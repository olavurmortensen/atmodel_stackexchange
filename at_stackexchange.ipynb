{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author-topic model Gensim tutorial at http://nbviewer.jupyter.org/github/rare-technologies/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb.\n",
    "\n",
    "This is not a tutorial, it is not user friendly, read at your own peril."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from imp import reload\n",
    "from pprint import pprint\n",
    "import os, shutil, re, random, logging, pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.matutils import sparse2full, hellinger\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.models import atmodel\n",
    "from gensim.models import ldamodel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure logging.\n",
    "\n",
    "log_dir = '../log_files/log.log'\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename=log_dir, mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_folder = '../../data/stackexchange/cooking/'\n",
    "input_fname = data_folder + 'Posts.xml'\n",
    "output_fname = '/tmp/cooking_docs.txt'\n",
    "tree = ET.parse(input_fname)\n",
    "root = tree.getroot()\n",
    "num_docs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts in dataset: 54566\n"
     ]
    }
   ],
   "source": [
    "post_ids = []\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        # This is the <posts> XML element.\n",
    "        continue\n",
    "    post_ids.append(int(item.get('Id')))\n",
    "\n",
    "print('Number of posts in dataset:', len(post_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doc_generator(root, num_docs=None):\n",
    "    '''\n",
    "    This generator parses the XML data, do some preliminary\n",
    "    pre-processing and yields the documents.\n",
    "    \n",
    "    '''\n",
    "    num_posts = 0\n",
    "    for post_id in post_ids:\n",
    "        post_text = ''\n",
    "        for i, item in enumerate(root.iter()):\n",
    "            if i == 0:\n",
    "                # This is the <posts> XML element.\n",
    "                continue\n",
    "            elif int(item.get('Id')) == post_id:\n",
    "                # This is the post.\n",
    "                post_text += item.get('Body')\n",
    "            elif item.get('ParentId') is not None and int(item.get('ParentId')) == post_id:\n",
    "                # This is an answer to the post.\n",
    "                post_text += item.get('Body')\n",
    "            else:\n",
    "                # Neither post \"post_id\" or answer to it.\n",
    "                continue\n",
    "\n",
    "            # Remove any HTML tags, such as <p>.\n",
    "            post_text = re.sub('<[^<]+?>', '', post_text)\n",
    "\n",
    "            # Replace any whitespace (newline, tabs, etc.) by a single space.\n",
    "            post_text = re.sub('\\s', ' ', post_text)\n",
    "\n",
    "        if num_docs is not None and num_posts >= num_docs:\n",
    "            break\n",
    "            \n",
    "        num_posts += 1\n",
    "        \n",
    "        yield post_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the default SpaCy NLP pipeline to process the documents in parallel.\n",
    "# Then use the output of the pipeline to transform the text.\n",
    "# Write the resulting text to a file.\n",
    "entity_freq = {}\n",
    "postid = 0\n",
    "with open(output_fname, 'w') as fid:\n",
    "    for doc in nlp.pipe(doc_generator(root, num_docs=num_docs), n_threads=4):\n",
    "        # Process post text.\n",
    "        \n",
    "        # NOTE: the doc_generator is probably the bottleneck here.\n",
    "        \n",
    "        ents = doc.ents  # Named entities.\n",
    "\n",
    "        # Keep only words (no numbers, no punctuation).\n",
    "        # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "        #doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "        \n",
    "        # Remove stopwords and punctuation, and lemmatized tokens.\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        #tokens = [str(token) for token in doc if not token.is_stop and not token.is_punct]\n",
    "        #tokens.extend([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "        \n",
    "        # Add named entities, but only if they are a compound of more than word.\n",
    "        #doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "        \n",
    "        #for entity in ents:\n",
    "        #    if entity_freq.get(entity):\n",
    "        #        entity_freq[entity] += 1\n",
    "        #    else:\n",
    "        #        entity_freq[entity] = 1\n",
    "        \n",
    "        # Write the doc to file.\n",
    "        fid.write(' '.join(tokens) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the tags of each post.\n",
    "postid = 0\n",
    "postid2tagname = dict()\n",
    "tag_set = set()\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        # This is the <posts> XML element.\n",
    "        continue\n",
    "    if item.get('Tags') is None:\n",
    "        # There are many posts with no tags.\n",
    "        continue\n",
    "    \n",
    "    if num_docs is not None and postid >= num_docs:\n",
    "        break\n",
    "\n",
    "    tags = item.get('Tags')\n",
    "    tags = re.findall('<(.+?)>', tags)\n",
    "    # NOTE: consider using a tag that is common for all posts, and/or\n",
    "    # a tag that is only for this particular post. \n",
    "    # NOTE: also consider including posts with no tags, and tag them with\n",
    "    # post ID or \"SUPER_TAG\", maybe both, maybe an extra \"NO_TAG\" tag.\n",
    "    #tags.append('SUPER_TAG')\n",
    "    tags.append('POST_ID' + str(postid))\n",
    "    postid2tagname[postid] = tags\n",
    "    for tag in tags:\n",
    "        tag_set.add(tag)\n",
    "\n",
    "    postid += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** these names aren't great, \"doc_generator\" and \"docs_generator\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def docs_generator(fname):\n",
    "    '''\n",
    "    This generator reads the processed text one line\n",
    "    at a time and yields documents (lists of words).\n",
    "    \n",
    "    '''\n",
    "    with open(fname, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()  # Remove newline (\"\\n\").\n",
    "            doc = line.split(' ')  # Split line text into words.\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olavur/Dropbox/my_folder/workstuff/DTU/thesis/code/gensim/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "docs = docs_generator(output_fname)\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "docs = docs_generator(output_fname)\n",
    "with open(output_fname + '.tmp', 'w') as fid:\n",
    "    for doc in docs:\n",
    "        for token in bigram[doc]:\n",
    "            if '_' in token:\n",
    "                doc.append(token)\n",
    "        fid.write(' '.join(doc) + '\\n')\n",
    "\n",
    "\n",
    "shutil.copyfile(output_fname + '.tmp', output_fname)\n",
    "os.remove(output_fname + '.tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "docs = docs_generator(output_fname)\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "# Disregarding stop words, this dataset has a very high number of low frequency words.\n",
    "max_freq = 0.5\n",
    "min_count = 5\n",
    "dictionary.filter_extremes(no_below=min_count, no_above=max_freq)\n",
    "\n",
    "dict0 = dictionary[0]  # This sort of \"initializes\" dictionary.id2token.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "#docs = docs_generator(output_fname)\n",
    "#corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "#num_docs = len(corpus)  # In case num_docs was set to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_generator(fname):\n",
    "    '''\n",
    "    This generator reads the processed text one line\n",
    "    at a time and yields BOW documents.\n",
    "    \n",
    "    '''\n",
    "    with open(fname, 'r') as fid:\n",
    "        for line in fid:\n",
    "            line = line.strip()  # Remove newline (\"\\n\").\n",
    "            doc = line.split(' ')  # Split line text into words.\n",
    "            yield dictionary.doc2bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus_generator(output_fname)\n",
    "\n",
    "# Serialize the corpus.\n",
    "MmCorpus.serialize('/tmp/corpus.mm', corpus)\n",
    "corpus = MmCorpus('/tmp/corpus.mm')\n",
    "\n",
    "num_docs = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagname2postid = atmodel.construct_author2doc(corpus, postid2tagname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimensionality:\n",
      "Number of authors: 237:\n",
      "Number of unique tokens: 434\n",
      "Number of documents: 100\n"
     ]
    }
   ],
   "source": [
    "print('Train data dimensionality:')\n",
    "print('Number of authors: %d:' % (len(tag_set)))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(atmodel)\n",
    "AuthorTopicModel = atmodel.AuthorTopicModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.8 s, sys: 12 ms, total: 1.81 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "num_topics = 10\n",
    "chunksize = num_docs + 1\n",
    "%time model = AuthorTopicModel(corpus=corpus, num_topics=num_topics, id2word=dictionary.id2token, \\\n",
    "                author2doc=tagname2postid, doc2author=postid2tagname, \\\n",
    "                chunksize=chunksize, passes=10, update_every=1, \\\n",
    "                alpha='symmetric', eta='symmetric', decay=0.5, offset=1.0, \\\n",
    "                eval_every=1, iterations=1, gamma_threshold=1e-10, \\\n",
    "                minimum_probability=0.01, random_state=0, \\\n",
    "                serialized=True, serialization_path='/tmp/model_serializer.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/model_serializer.mm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ed2dccf985a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/model_serializer.mm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/model_serializer.mm'"
     ]
    }
   ],
   "source": [
    "os.remove('/tmp/model_serializer.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.030*\"spice\" + 0.016*\"bread\" + 0.015*\"use\" + 0.013*\"dry\" + 0.012*\"cook\" + 0.012*\"good\" + 0.011*\"not\" + 0.010*\"have\" + 0.010*\"thing\" + 0.010*\"way\"'),\n",
       " (1,\n",
       "  '0.040*\"1\" + 0.032*\"half\" + 0.024*\"cup\" + 0.018*\"convert\" + 0.017*\"tbsp\" + 0.017*\"cream\" + 0.017*\"2\" + 0.016*\"egg\" + 0.015*\"bacon\" + 0.015*\"recipe\"'),\n",
       " (2,\n",
       "  '0.031*\"recipe\" + 0.024*\"not\" + 0.023*\"use\" + 0.019*\"\\'\" + 0.017*\"good\" + 0.015*\"ingredient\" + 0.015*\"like\" + 0.013*\"add\" + 0.012*\"list\" + 0.012*\"egg\"'),\n",
       " (3,\n",
       "  '0.029*\"spice\" + 0.023*\"not\" + 0.021*\"cook\" + 0.020*\"food\" + 0.018*\"add\" + 0.017*\"use\" + 0.017*\"raw\" + 0.013*\"like\" + 0.012*\"\\'\" + 0.012*\"butter\"'),\n",
       " (4,\n",
       "  '0.031*\"not\" + 0.027*\"bread\" + 0.023*\"good\" + 0.020*\"use\" + 0.017*\"\\'\" + 0.013*\"meat\" + 0.012*\"store\" + 0.011*\"long\" + 0.011*\"way\" + 0.011*\"bag\"'),\n",
       " (5,\n",
       "  '0.052*\"egg\" + 0.017*\"food\" + 0.017*\"add\" + 0.016*\"not\" + 0.014*\"\\'\" + 0.014*\"center\" + 0.012*\"use\" + 0.012*\"fresh\" + 0.011*\"recipe\" + 0.011*\"rice\"'),\n",
       " (6,\n",
       "  '0.033*\"oil\" + 0.031*\"grill\" + 0.024*\"pan\" + 0.018*\"\\'\" + 0.018*\"cook\" + 0.017*\"use\" + 0.017*\"food\" + 0.015*\"not\" + 0.013*\"egg\" + 0.013*\"time\"'),\n",
       " (7,\n",
       "  '0.036*\"add\" + 0.033*\"tomato\" + 0.030*\"sauce\" + 0.022*\"cook\" + 0.017*\"onion\" + 0.017*\"not\" + 0.016*\"sugar\" + 0.015*\"rice\" + 0.015*\"use\" + 0.014*\"\\'re\"'),\n",
       " (8,\n",
       "  '0.021*\"not\" + 0.018*\"bag\" + 0.017*\"use\" + 0.016*\"bread\" + 0.015*\"add\" + 0.012*\"bake\" + 0.012*\"place\" + 0.011*\"meat\" + 0.011*\"baking\" + 0.011*\"cook\"'),\n",
       " (9,\n",
       "  '0.041*\"egg\" + 0.032*\"butter\" + 0.021*\"water\" + 0.020*\"add\" + 0.018*\"not\" + 0.017*\"use\" + 0.013*\"cook\" + 0.013*\"\\'\" + 0.011*\"get\" + 0.011*\"cream\"')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    tag = random.choice(list(train_tagname2postid.keys()))\n",
    "    while tag[:7] == 'POST_ID':\n",
    "        tag = random.choice(list(train_tagname2postid.keys()))\n",
    "    print('\\n%s' % tag)\n",
    "    print('#Docs:', len(model.author2doc[tag]))\n",
    "    pprint(model.get_author_topics(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.show_topic(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag = 'baking'\n",
    "print('%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'eggs'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'pasta'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'herbs'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'beef'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n",
    "\n",
    "tag = 'salmon'\n",
    "print('\\n%s' % tag)\n",
    "print('#Docs:', len(model.author2doc[tag]))\n",
    "pprint(model.get_author_topics(tag))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity(vec1, vec2):\n",
    "    dist = hellinger(sparse2full(vec1, num_topics), sparse2full(vec2, num_topics))\n",
    "    sim = 1.0 / (1.0 + dist)\n",
    "    return sim\n",
    "\n",
    "def get_sims(vec, tag_vecs):\n",
    "    sims = [similarity(vec, vec2) for vec2 in tag_vecs]\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_vecs = [model.get_author_topics(tag, minimum_probability=0.0) for tag in train_tag_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2tag = dict(zip(range(len(train_tag_set)), list(train_tag_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag = random.choice(list(train_tagname2postid.keys()))\n",
    "while tag[:7] == 'POST_ID':\n",
    "    tag = random.choice(list(train_tagname2postid.keys()))\n",
    "sims = get_sims(model.get_author_topics(tag, minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = get_sims(model.get_author_topics('beef', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = get_sims(model.get_author_topics('baking', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = get_sims(model.get_author_topics('salmon', minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims = [(id2tag[elem[0]], elem[1]) for elem in enumerate(sims) if not id2tag[elem[0]][:7] == 'POST_ID']\n",
    "sims_df = pd.DataFrame(sims, columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the tag of a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=None, num_topics=num_topics, id2word=dictionary.id2token)\n",
    "lda.state.sstats = model.state.sstats\n",
    "lda.iterations = 100  # Make sure training converges on document when calling lda[doc]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "postid = 1\n",
    "doc = test_corpus[postid]\n",
    "print('Post tags:\\n', test_postid2tagname[postid])\n",
    "print('Post body:\\n', test_docs[postid])\n",
    "\n",
    "for tag in test_postid2tagname[postid]:\n",
    "    if tag not in train_tag_set:\n",
    "        print('Tag \"', tag, '\" not in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sims = get_sims(lda.get_document_topics(doc, minimum_probability=0.0), tag_vecs)\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_threshold = 0.6\n",
    "pred = sims_df.loc[sims_df.Score > pred_threshold]\n",
    "pred_tags = list(pred.Tag)\n",
    "pred_prob = list(pred.Score)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for tag in pred_tags:\n",
    "    if tag in test_postid2tagname[postid]:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for tag in test_postid2tagname[postid]:\n",
    "    if tag not in pred_tags:\n",
    "        fn += 1\n",
    "        \n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "if precision + recall == 0:\n",
    "    f1_score = 0.0\n",
    "else:\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print('F1 score: ', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a similarity object for the transformed corpus.\n",
    "index = MatrixSimilarity(model[list(train_tag_set)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get similarities to some tag.\n",
    "tag_name = 'baking'\n",
    "sims = index[model[tag_name]]\n",
    "\n",
    "# Print the most similar tags.\n",
    "sims_df = pd.DataFrame([(id2tag[elem[0]], elem[1]) for elem in enumerate(sims)], columns=['Tag', 'Score'])\n",
    "sims_df.sort_values('Score', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time lda = LdaModel(corpus, num_topics=10, id2word=dictionary.id2token, iterations=1, \\\n",
    "                     passes=100, eval_every=0, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = ET.parse(input_fname)\n",
    "postid2tagname = dict()\n",
    "postid = 0\n",
    "posts = []\n",
    "tag_set = set()\n",
    "for i, item in enumerate(root.iter()):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if item.get('Tags') is not None:\n",
    "        tags = item.get('Tags')\n",
    "        tags = re.findall('<(.+?)>', tags)\n",
    "        # NOTE: consider using a tag that is common for all posts, or\n",
    "        # a tag that is only for this particular post.\n",
    "        #tags.append('SUPER_TAG')\n",
    "        #tags.append('POST_ID' + str(postid))\n",
    "        postid2tagname[postid] = tags\n",
    "        posts.append(item.get('Body'))\n",
    "        for tag in tags:\n",
    "            tag_set.add(tag)\n",
    "        postid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_posts = len(posts)\n",
    "docs = []\n",
    "for post in posts[:]:\n",
    "    # Remove any HTML tags, such as <p>.\n",
    "    text = re.sub('<[^<]+?>', '', post)\n",
    "    doc = nlp(text)\n",
    "    ents = doc.ents  # Named entities.\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha]\n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in STOPWORDS]\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
